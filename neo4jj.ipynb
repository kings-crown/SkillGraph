{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from pathlib import Path\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084eeeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = Path(\".env\").resolve()         \n",
    "load_dotenv(env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bebc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from difflib import SequenceMatcher\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e353730",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(refresh_schema=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edcdc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # Optional dependency guard for clearer import errors downstream\n",
    "    from langchain_neo4j import Neo4jGraph\n",
    "except ImportError as exc:  # pragma: no cover - provides actionable hint if package missing\n",
    "    raise ImportError(\n",
    "        \"langchain-neo4j is required. Install it with `pip install langchain-neo4j`.\"\n",
    "    ) from exc\n",
    "\n",
    "try:  # Same idea for langchain OpenAI helpers\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "except ImportError as exc:  # pragma: no cover - provides actionable hint if package missing\n",
    "    raise ImportError(\n",
    "        \"langchain-openai is required. Install it with `pip install langchain-openai`.\"\n",
    "    ) from exc\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53306b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_environment() -> None:\n",
    "    env_path = Path(__file__).resolve().parent / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "    else:\n",
    "        load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db90b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SkillRecord:\n",
    "    \"\"\"Lightweight container for skill descriptors returned from the graph.\"\"\"\n",
    "\n",
    "    element_id: str\n",
    "    name: str\n",
    "    description: str\n",
    "    importance: float\n",
    "    level: float\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        \"\"\"Canonical text representation used for embedding comparisons.\"\"\"\n",
    "        if self.description:\n",
    "            return f\"{self.name}: {self.description}\"\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class SkillGraphClient:\n",
    "    \"\"\"Wrapper around Neo4j queries used by the RAG pipeline.\"\"\"\n",
    "\n",
    "    CODE_PATTERN = re.compile(r\"^\\d{2}-\\d{4}\\.\\d{2}$\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph: Optional[Neo4jGraph] = None,\n",
    "        *,\n",
    "        database: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        _load_environment()\n",
    "\n",
    "        if graph is not None:\n",
    "            self.graph = graph\n",
    "        else:\n",
    "            uri = os.getenv(\"NEO4J_URI\")\n",
    "            username = os.getenv(\"NEO4J_USERNAME\")\n",
    "            password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "            database = database or os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "            if not all([uri, username, password]):\n",
    "                raise EnvironmentError(\n",
    "                    \"Neo4j credentials are missing. Set NEO4J_URI, NEO4J_USERNAME, and NEO4J_PASSWORD.\"\n",
    "                )\n",
    "\n",
    "            self.graph = Neo4jGraph(url=uri, username=username, password=password, database=database)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Occupation lookups\n",
    "    # ---------------------------------------------------------------------\n",
    "    def resolve_occupation(self, title_or_code: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Return candidate occupations for a user supplied title or O*NET code.\"\"\"\n",
    "        candidate_title = title_or_code.strip()\n",
    "        params: Dict[str, Any] = {\"title\": candidate_title, \"limit\": limit * 5}\n",
    "\n",
    "        if self.CODE_PATTERN.match(candidate_title):\n",
    "            logger.debug(\"Resolving occupation by exact code: %s\", candidate_title)\n",
    "            query = (\n",
    "                \"MATCH (o:Occupation {code: $code})\\n\"\n",
    "                \"RETURN o.code AS code, o.title AS title, o.description AS description\"\n",
    "            )\n",
    "            return self.graph.query(query, {\"code\": candidate_title})\n",
    "\n",
    "        query = (\n",
    "            \"MATCH (o:Occupation)\\n\"\n",
    "            \"WHERE toLower(o.title) CONTAINS toLower($title)\\n\"\n",
    "            \"   OR toLower($title) CONTAINS toLower(o.title)\\n\"\n",
    "            \"RETURN o.code AS code, o.title AS title, o.description AS description\\n\"\n",
    "            \"LIMIT toInteger($limit)\"\n",
    "        )\n",
    "        records = self.graph.query(query, params)\n",
    "\n",
    "        if not records:\n",
    "            logger.debug(\"No direct substring match for '%s'; broadening search via all occupations.\", candidate_title)\n",
    "            # Wider scan â€“ still limited to keep latency reasonable\n",
    "            broad_query = (\n",
    "                \"MATCH (o:Occupation)\\n\"\n",
    "                \"RETURN o.code AS code, o.title AS title, o.description AS description\\n\"\n",
    "                \"LIMIT toInteger($limit)\"\n",
    "            )\n",
    "            records = self.graph.query(broad_query, {\"limit\": limit * 20})\n",
    "\n",
    "        scored: List[Dict[str, Any]] = []\n",
    "        for record in records:\n",
    "            ratio = SequenceMatcher(None, candidate_title.lower(), record[\"title\"].lower()).ratio()\n",
    "            scored.append({**record, \"score\": ratio})\n",
    "\n",
    "        scored.sort(key=lambda item: item[\"score\"], reverse=True)\n",
    "        return scored[:limit]\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Skill retrieval\n",
    "    # ---------------------------------------------------------------------\n",
    "    def occupation_skills(self, occupation_code: str, *, limit: Optional[int] = None) -> List[SkillRecord]:\n",
    "        \"\"\"Fetch skill descriptors tied to an occupation ordered by importance.\"\"\"\n",
    "        params = {\"code\": occupation_code}\n",
    "        cypher = (\n",
    "            \"MATCH (o:Occupation {code: $code})-[r:REQUIRES_SKILL]->(skill:ContentElement)\\n\"\n",
    "            \"WHERE 'Skill' IN labels(skill)\\n\"\n",
    "            \"RETURN skill.element_id AS element_id,\\n\"\n",
    "            \"       skill.name AS name,\\n\"\n",
    "            \"       coalesce(skill.description, '') AS description,\\n\"\n",
    "            \"       toFloat(coalesce(r.importance, 0)) AS importance,\\n\"\n",
    "            \"       toFloat(coalesce(r.level, 0)) AS level\\n\"\n",
    "            \"ORDER BY importance DESC\"\n",
    "        )\n",
    "\n",
    "        if limit:\n",
    "            cypher += \"\\nLIMIT toInteger($limit)\"\n",
    "            params[\"limit\"] = limit\n",
    "\n",
    "        records = self.graph.query(cypher, params)\n",
    "        return [\n",
    "            SkillRecord(\n",
    "                element_id=record[\"element_id\"],\n",
    "                name=record[\"name\"],\n",
    "                description=record[\"description\"],\n",
    "                importance=float(record[\"importance\"] or 0.0),\n",
    "                level=float(record[\"level\"] or 0.0),\n",
    "            )\n",
    "            for record in records\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f743d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillMatcher:\n",
    "    \"\"\"Embeds and aligns free-form user skills against graph descriptors.\"\"\"\n",
    "\n",
    "    def __init__(self, embedder: Optional[OpenAIEmbeddings] = None) -> None:\n",
    "        _load_environment()\n",
    "        self.embedder = embedder or OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        self._skill_embedding_cache: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    def match(\n",
    "        self,\n",
    "        user_skill_texts: Sequence[str],\n",
    "        graph_skills: Sequence[SkillRecord],\n",
    "        *,\n",
    "        similarity_threshold: float = 0.7,\n",
    "    ) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Return skills the user already covers and those likely missing.\"\"\"\n",
    "        if not graph_skills:\n",
    "            return {\"matched\": [], \"missing\": []}\n",
    "\n",
    "        if not user_skill_texts:\n",
    "            missing_payload = [\n",
    "                {\n",
    "                    \"skill\": skill,\n",
    "                    \"score\": None,\n",
    "                    \"matched_user_skill\": None,\n",
    "                }\n",
    "                for skill in graph_skills\n",
    "            ]\n",
    "            return {\"matched\": [], \"missing\": missing_payload}\n",
    "\n",
    "        # Deduplicate user skill phrases while keeping order\n",
    "        seen = set()\n",
    "        normalised_user_skills = []\n",
    "        for skill_text in user_skill_texts:\n",
    "            key = skill_text.strip().lower()\n",
    "            if key and key not in seen:\n",
    "                normalised_user_skills.append(skill_text.strip())\n",
    "                seen.add(key)\n",
    "\n",
    "        user_vectors = self.embedder.embed_documents(normalised_user_skills)\n",
    "        user_matrix = np.asarray(user_vectors, dtype=np.float32)\n",
    "\n",
    "        skill_matrix = []\n",
    "        for skill in graph_skills:\n",
    "            vector = self._cached_skill_embedding(skill)\n",
    "            skill_matrix.append(vector)\n",
    "        skill_matrix_np = np.asarray(skill_matrix, dtype=np.float32)\n",
    "\n",
    "        user_norms = np.linalg.norm(user_matrix, axis=1, keepdims=True)\n",
    "        skill_norms = np.linalg.norm(skill_matrix_np, axis=1, keepdims=True)\n",
    "        user_norms[user_norms == 0] = 1e-12\n",
    "        skill_norms[skill_norms == 0] = 1e-12\n",
    "        similarity_matrix = (user_matrix / user_norms) @ (skill_matrix_np / skill_norms).T\n",
    "\n",
    "        best_user_indices = np.argmax(similarity_matrix, axis=0)\n",
    "        best_scores = similarity_matrix[best_user_indices, np.arange(similarity_matrix.shape[1])]\n",
    "\n",
    "        matched: List[Dict[str, Any]] = []\n",
    "        missing: List[Dict[str, Any]] = []\n",
    "        for idx, skill in enumerate(graph_skills):\n",
    "            score = float(best_scores[idx])\n",
    "            best_user_skill = normalised_user_skills[int(best_user_indices[idx])]\n",
    "            payload = {\n",
    "                \"skill\": skill,\n",
    "                \"score\": round(score, 3),\n",
    "                \"matched_user_skill\": best_user_skill,\n",
    "            }\n",
    "            if score >= similarity_threshold:\n",
    "                matched.append(payload)\n",
    "            else:\n",
    "                payload[\"matched_user_skill\"] = None\n",
    "                missing.append(payload)\n",
    "\n",
    "        matched.sort(key=lambda item: (item[\"skill\"].importance, item[\"score\"]), reverse=True)\n",
    "        missing.sort(key=lambda item: item[\"skill\"].importance, reverse=True)\n",
    "        return {\"matched\": matched, \"missing\": missing}\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _cached_skill_embedding(self, skill: SkillRecord) -> np.ndarray:\n",
    "        cached = self._skill_embedding_cache.get(skill.element_id)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        embedding = self.embedder.embed_query(skill.text)\n",
    "        vector = np.asarray(embedding, dtype=np.float32)\n",
    "        self._skill_embedding_cache[skill.element_id] = vector\n",
    "        return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillGraphRAG:\n",
    "    \"\"\"High-level orchestrator that produces grounded skill profiles.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        graph_client: Optional[SkillGraphClient] = None,\n",
    "        matcher: Optional[SkillMatcher] = None,\n",
    "        llm: Optional[ChatOpenAI] = None,\n",
    "        similarity_threshold: float = 0.72,\n",
    "    ) -> None:\n",
    "        _load_environment()\n",
    "        self.graph_client = graph_client or SkillGraphClient()\n",
    "        self.matcher = matcher or SkillMatcher()\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "        self.llm = llm or ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "    def generate_skill_profile(\n",
    "        self,\n",
    "        parsed_resume: Dict[str, Any],\n",
    "        target_roles: Sequence[str],\n",
    "        *,\n",
    "        qna: Optional[Sequence[Dict[str, str]]] = None,\n",
    "        max_skills_per_role: int = 25,\n",
    "        summarise: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Build a structured, graph-grounded skill profile for the user.\"\"\"\n",
    "        technical_skills = parsed_resume.get(\"technical_skills\", [])\n",
    "        soft_skills = parsed_resume.get(\"soft_skills\", [])\n",
    "        user_skill_texts = [*technical_skills, *soft_skills]\n",
    "\n",
    "        role_profiles: List[Dict[str, Any]] = []\n",
    "        for requested_role in target_roles:\n",
    "            occupation_matches = self.graph_client.resolve_occupation(requested_role, limit=3)\n",
    "            if not occupation_matches:\n",
    "                role_profiles.append(\n",
    "                    {\n",
    "                        \"requested_role\": requested_role,\n",
    "                        \"occupation_match\": None,\n",
    "                        \"skills_covered\": [],\n",
    "                        \"skill_gaps\": [],\n",
    "                        \"all_skills\": [],\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            best_match = occupation_matches[0]\n",
    "            occ_code = best_match[\"code\"]\n",
    "            graph_skills = self.graph_client.occupation_skills(occ_code, limit=max_skills_per_role)\n",
    "            matching = self.matcher.match(user_skill_texts, graph_skills, similarity_threshold=self.similarity_threshold)\n",
    "\n",
    "            role_profiles.append(\n",
    "                {\n",
    "                    \"requested_role\": requested_role,\n",
    "                    \"occupation_match\": best_match,\n",
    "                    \"alternate_matches\": occupation_matches[1:],\n",
    "                    \"skills_covered\": matching[\"matched\"],\n",
    "                    \"skill_gaps\": matching[\"missing\"],\n",
    "                    \"all_skills\": graph_skills,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        analysis = None\n",
    "        if summarise:\n",
    "            analysis = self._summarise_profile(role_profiles, user_skill_texts, qna=qna, parsed_resume=parsed_resume)\n",
    "\n",
    "        return {\n",
    "            \"user_skills\": user_skill_texts,\n",
    "            \"profiles\": role_profiles,\n",
    "            \"analysis\": analysis,\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _summarise_profile(\n",
    "        self,\n",
    "        role_profiles: Sequence[Dict[str, Any]],\n",
    "        user_skills: Sequence[str],\n",
    "        *,\n",
    "        qna: Optional[Sequence[Dict[str, str]]] = None,\n",
    "        parsed_resume: Optional[Dict[str, Any]] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Create a narrative summary grounded in the knowledge graph facts.\"\"\"\n",
    "        summary_payload = {\n",
    "            \"user_skills\": list(user_skills),\n",
    "            \"qna\": list(qna) if qna else [],\n",
    "            \"profiles\": [\n",
    "                {\n",
    "                    \"requested_role\": profile[\"requested_role\"],\n",
    "                    \"occupation_match\": profile[\"occupation_match\"],\n",
    "                    \"top_skills_covered\": [\n",
    "                        {\n",
    "                            \"name\": item[\"skill\"].name,\n",
    "                            \"importance\": item[\"skill\"].importance,\n",
    "                            \"score\": item[\"score\"],\n",
    "                        }\n",
    "                        for item in profile[\"skills_covered\"][:8]\n",
    "                    ],\n",
    "                    \"top_skill_gaps\": [\n",
    "                        {\n",
    "                            \"name\": item[\"skill\"].name,\n",
    "                            \"importance\": item[\"skill\"].importance,\n",
    "                        }\n",
    "                        for item in profile[\"skill_gaps\"][:8]\n",
    "                    ],\n",
    "                }\n",
    "                for profile in role_profiles\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        if parsed_resume:\n",
    "            summary_payload[\"resume_snapshot\"] = {\n",
    "                \"experience\": parsed_resume.get(\"experience\"),\n",
    "                \"education\": parsed_resume.get(\"education\"),\n",
    "                \"summary\": parsed_resume.get(\"summary\"),\n",
    "            }\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are an AI career coach that must ground every assessment in the provided \"\n",
    "                    \"knowledge graph facts. Do not hallucinate new skills. When discussing gaps, reference \"\n",
    "                    \"the occupation title and highlight why the skill matters.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    \"Using the structured context below, craft a concise skill profile that\\n\"\n",
    "                    \"1. Highlights the strongest validated skills.\\n\"\n",
    "                    \"2. Lists the most critical skill gaps per role with actionable guidance.\\n\"\n",
    "                    \"3. Mentions any notable preferences from the questionnaire if available.\\n\"\n",
    "                    \"Context: \" + json.dumps(summary_payload, indent=2)\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skill_profile(\n",
    "    parsed_resume: Dict[str, Any],\n",
    "    target_roles: Sequence[str],\n",
    "    *,\n",
    "    qna: Optional[Sequence[Dict[str, str]]] = None,\n",
    "    summarise: bool = True,\n",
    "    similarity_threshold: float = 0.72,\n",
    "    max_skills_per_role: int = 25,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Convenience helper to run the end-to-end SkillGraph RAG pipeline.\"\"\"\n",
    "    rag = SkillGraphRAG(similarity_threshold=similarity_threshold)\n",
    "    return rag.generate_skill_profile(\n",
    "        parsed_resume,\n",
    "        target_roles,\n",
    "        qna=qna,\n",
    "        max_skills_per_role=max_skills_per_role,\n",
    "        summarise=summarise,\n",
    "    )\n",
    "\n",
    "\n",
    "def _demo() -> None:  # pragma: no cover - utility for manual verification\n",
    "    \"\"\"Quick demonstration when executing this module directly.\"\"\"\n",
    "    sample_resume = {\n",
    "        \"technical_skills\": [\"Python\", \"Data Analysis\", \"Machine Learning\", \"SQL\"],\n",
    "        \"soft_skills\": [\"Collaboration\", \"Communication\"],\n",
    "        \"experience\": \"2 years as a junior data analyst focusing on reporting and dashboarding.\",\n",
    "        \"education\": [\"B.S. in Information Systems\"],\n",
    "    }\n",
    "    target_roles = [\"Data Scientist\", \"Business Intelligence Analyst\"]\n",
    "\n",
    "    rag = SkillGraphRAG()\n",
    "    profile = rag.generate_skill_profile(sample_resume, target_roles, summarise=True)\n",
    "\n",
    "    print(json.dumps({\n",
    "        \"profiles\": profile[\"profiles\"],\n",
    "        \"analysis\": profile[\"analysis\"],\n",
    "    }, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  # pragma: no cover - manual execution guard\n",
    "    try:\n",
    "        _demo()\n",
    "    except Exception as exc:  # Basic diagnostic when running ad hoc from CLI\n",
    "        logger.error(\"Demo execution failed: %s\", exc)\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
